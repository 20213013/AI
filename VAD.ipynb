{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터셋 경로\n",
    "mix_path = \"/data/cjchun/ejkim/SpeechEnhancement(25.1)/mix\"\n",
    "clean_path = \"/data/cjchun/ejkim/SpeechEnhancement(25.1)/clean\"\n",
    "\n",
    "# 데이터셋 분석 함수\n",
    "def analyze_dataset(dataset_path):\n",
    "    file_lengths = []\n",
    "    amplitudes = []\n",
    "\n",
    "    print(f\"Analyzing dataset: {dataset_path}\")\n",
    "    for file_name in tqdm(os.listdir(dataset_path)):\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "        if file_path.endswith(\".wav\"):\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            file_lengths.append(len(audio) / sr)  # Length in seconds\n",
    "            amplitudes.append(audio)\n",
    "\n",
    "    # Length statistics\n",
    "    min_length = np.min(file_lengths)\n",
    "    max_length = np.max(file_lengths)\n",
    "    avg_length = np.mean(file_lengths)\n",
    "\n",
    "    # Amplitude statistics\n",
    "    all_amplitudes = np.concatenate(amplitudes)\n",
    "    min_amplitude = np.min(all_amplitudes)\n",
    "    max_amplitude = np.max(all_amplitudes)\n",
    "    avg_amplitude = np.mean(all_amplitudes)\n",
    "\n",
    "    return {\n",
    "        \"num_files\": len(file_lengths),\n",
    "        \"min_length\": min_length,\n",
    "        \"max_length\": max_length,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"min_amplitude\": min_amplitude,\n",
    "        \"max_amplitude\": max_amplitude,\n",
    "        \"avg_amplitude\": avg_amplitude,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분석\n",
    "mix_stats = analyze_dataset(mix_path)\n",
    "clean_stats = analyze_dataset(clean_path)\n",
    "\n",
    "# 분석 결과 출력\n",
    "def print_stats(stats, name):\n",
    "    print(f\"\\n{name} Dataset Statistics:\")\n",
    "    print(f\"- Number of files: {stats['num_files']}\")\n",
    "    print(f\"- Length (min/max/avg): {stats['min_length']:.2f}s / {stats['max_length']:.2f}s / {stats['avg_length']:.2f}s\")\n",
    "    print(f\"- Amplitude (min/max/avg): {stats['min_amplitude']:.4f} / {stats['max_amplitude']:.4f} / {stats['avg_amplitude']:.4f}\")\n",
    "\n",
    "print_stats(mix_stats, \"Mix\")\n",
    "print_stats(clean_stats, \"Clean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 인덱스 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 데이터셋 경로\n",
    "mix_dataset_path = \"/data/cjchun/ejkim/SpeechEnhancement(25.1)/mix\"\n",
    "clean_dataset_path = \"/data/cjchun/ejkim/SpeechEnhancement(25.1)/clean\"\n",
    "\n",
    "# 파일 리스트 로드\n",
    "mix_files = sorted(os.listdir(mix_dataset_path))\n",
    "clean_files = sorted(os.listdir(clean_dataset_path))\n",
    "\n",
    "# 매칭된 파일 생성\n",
    "matched_mix_files = []\n",
    "matched_clean_files = []\n",
    "\n",
    "# 매칭 기준: 파일 이름으로 매칭\n",
    "for clean_file in clean_files:\n",
    "    if clean_file in mix_files:\n",
    "        matched_mix_files.append(os.path.join(mix_dataset_path, clean_file))\n",
    "        matched_clean_files.append(os.path.join(clean_dataset_path, clean_file))\n",
    "\n",
    "# 매칭 결과 확인\n",
    "print(f\"매칭된 mix 파일 수: {len(matched_mix_files)}\")\n",
    "print(f\"매칭된 clean 파일 수: {len(matched_clean_files)}\")\n",
    "\n",
    "# 파일 수가 동일한지 확인\n",
    "assert len(matched_mix_files) == len(matched_clean_files), \"매칭된 mix와 clean 파일 수가 다릅니다!\"\n",
    "print(\"매칭된 mix와 clean 파일 수가 동일합니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STFT 파라미터\n",
    "n_fft = 1024  # FFT window size\n",
    "hop_length = 256  # Hop length\n",
    "win_length = 1024  # Window length\n",
    "window = torch.hann_window(win_length).to(device)\n",
    "\n",
    "# Magnitude와 Phase를 저장할 리스트\n",
    "mix_magnitude_list = []\n",
    "mix_phase_list = []\n",
    "clean_magnitude_list = []\n",
    "clean_phase_list = []\n",
    "\n",
    "# STFT 수행 함수\n",
    "def compute_stft(file_path, n_fft, hop_length, win_length, window):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    # STFT 수행\n",
    "    stft_output = torch.stft(\n",
    "        waveform,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=window,\n",
    "        return_complex=True\n",
    "    )\n",
    "    magnitude = torch.abs(stft_output)\n",
    "    phase = torch.angle(stft_output)\n",
    "\n",
    "    return magnitude, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매칭된 mix와 clean 데이터셋에 대해 STFT 수행\n",
    "for mix_file, clean_file in zip(matched_mix_files, matched_clean_files):\n",
    "    mix_magnitude, mix_phase = compute_stft(mix_file, n_fft, hop_length, win_length, window)\n",
    "    clean_magnitude, clean_phase = compute_stft(clean_file, n_fft, hop_length, win_length, window)\n",
    "\n",
    "    mix_magnitude_list.append(mix_magnitude)\n",
    "    mix_phase_list.append(mix_phase)\n",
    "    clean_magnitude_list.append(clean_magnitude)\n",
    "    clean_phase_list.append(clean_phase)\n",
    "\n",
    "# Magnitude와 Phase 시각화 (첫 번째 샘플)\n",
    "def plot_spectrogram(magnitude, title):\n",
    "    magnitude_db = 20 * torch.log10(magnitude + 1e-6).cpu().numpy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(magnitude_db[0], origin='lower', aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# mix 데이터의 Magnitude 시각화\n",
    "plot_spectrogram(mix_magnitude_list[0], \"Mix Magnitude Spectrogram\")\n",
    "\n",
    "# clean 데이터의 Magnitude 시각화\n",
    "plot_spectrogram(clean_magnitude_list[0], \"Clean Magnitude Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mel 변환 파라미터\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_mels = 128\n",
    "\n",
    "# MelSpectrogram 변환기 정의\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000,  # 샘플 레이트 (고정)\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mels=n_mels\n",
    ").to(device)\n",
    "\n",
    "# dB 변환기 정의\n",
    "db_transform = T.AmplitudeToDB(stype=\"power\").to(device)\n",
    "\n",
    "# Mel 변환 및 dB 변환 수행\n",
    "mix_mel_list = []\n",
    "clean_mel_list = []\n",
    "\n",
    "print(\"Converting matched datasets to Mel-spectrograms...\")\n",
    "for mix, clean in zip(matched_mix_list, matched_clean_list):\n",
    "    mix = mix.to(device)\n",
    "    clean = clean.to(device)\n",
    "\n",
    "    # Mel 변환\n",
    "    mix_mel = mel_transform(mix)\n",
    "    clean_mel = mel_transform(clean)\n",
    "\n",
    "    # dB 변환 (시각화용)\n",
    "    mix_mel_db = db_transform(mix_mel)\n",
    "    clean_mel_db = db_transform(clean_mel)\n",
    "\n",
    "    # 리스트에 추가\n",
    "    mix_mel_list.append(mix_mel_db.cpu())\n",
    "    clean_mel_list.append(clean_mel_db.cpu())\n",
    "\n",
    "print(\"Mel-spectrogram 변환 완료.\")\n",
    "\n",
    "# 리스트를 텐서로 변환\n",
    "mix_mel_list = torch.stack(mix_mel_list)\n",
    "clean_mel_list = torch.stack(clean_mel_list)\n",
    "\n",
    "# 샘플 데이터 시각화 (격자 형태)\n",
    "def plot_mel_spectrograms(mel_list, title, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        mel = mel_list[i].squeeze().numpy()\n",
    "        axes[i].imshow(mel, origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "        axes[i].set_title(f\"{title} {i+1}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mix 및 Clean Mel-spectrogram 시각화\n",
    "plot_mel_spectrograms(mix_mel_list, \"Mix Mel\", num_samples=5)\n",
    "plot_mel_spectrograms(clean_mel_list, \"Clean Mel\", num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# UNet 모델 정의\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# 데이터 준비\n",
    "mix_mel_tensor = mix_mel_list.unsqueeze(1)  # (Batch, Channels, Time, Mel)\n",
    "clean_mel_tensor = clean_mel_list.unsqueeze(1)  # (Batch, Channels, Time, Mel)\n",
    "\n",
    "dataset = TensorDataset(mix_mel_tensor, clean_mel_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 가중치 초기화\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# 학습 루프\n",
    "train_losses = []\n",
    "print(\"Training UNet model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for mix_batch, clean_batch in dataloader:\n",
    "        mix_batch = mix_batch.to(device)\n",
    "        clean_batch = clean_batch.to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        output = model(mix_batch)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(output, clean_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에포크 평균 손실 계산 및 저장\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# 학습 손실 시각화\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"UNet Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"UNet 학습 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRM MASK & Wiener Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# IRM 마스크 생성 함수\n",
    "def compute_irm(clean_magnitude, mix_magnitude, eps=1e-8):\n",
    "    \"\"\"Ideal Ratio Mask 계산\"\"\"\n",
    "    return clean_magnitude / (mix_magnitude + eps)\n",
    "\n",
    "# Wiener 필터 적용 함수\n",
    "def apply_wiener_filter(mix_stft, enhanced_stft, eps=1e-8):\n",
    "    \"\"\"Wiener 필터 적용\"\"\"\n",
    "    power_mix = np.abs(mix_stft) ** 2\n",
    "    power_enhanced = np.abs(enhanced_stft) ** 2\n",
    "    gain = power_enhanced / (power_mix + eps)\n",
    "    return gain * mix_stft\n",
    "\n",
    "# UNet 출력으로 IRM 마스크 적용\n",
    "print(\"Applying IRM and Wiener filter...\")\n",
    "\n",
    "# Mel-spectrogram 데이터에서 magnitude 추출\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    mix_batch = mix_mel_list.unsqueeze(1).to(device)\n",
    "    enhanced_mel = model(mix_batch).squeeze(1)  # UNet 출력\n",
    "\n",
    "# IRM 마스크 생성\n",
    "mix_magnitude = mix_mel_list.cpu().numpy()\n",
    "clean_magnitude = clean_mel_list.cpu().numpy()\n",
    "irm_mask = compute_irm(clean_magnitude, mix_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRM 마스크 적용\n",
    "enhanced_with_irm = irm_mask * mix_magnitude\n",
    "\n",
    "# Wiener 필터 적용\n",
    "mix_stft, mix_phase = mix_stft_list, mix_phase_list\n",
    "enhanced_stft = torch.stft(\n",
    "    torch.tensor(enhanced_with_irm).to(device),\n",
    "    n_fft=512,\n",
    "    hop_length=256,\n",
    "    win_length=512,\n",
    "    return_complex=True,\n",
    ").cpu().numpy()\n",
    "enhanced_wiener = apply_wiener_filter(mix_stft, enhanced_stft)\n",
    "\n",
    "# Wiener 필터 결과를 Mel-spectrogram으로 변환\n",
    "enhanced_wiener_mel = torch.log1p(torch.tensor(np.abs(enhanced_wiener))).numpy()\n",
    "\n",
    "# 시각화: 원본, IRM 적용, Wiener 필터 적용 결과\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Mix Mel\")\n",
    "plt.imshow(mix_mel_list[0], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"IRM Applied Mel\")\n",
    "plt.imshow(enhanced_with_irm[0], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Wiener Filter Applied Mel\")\n",
    "plt.imshow(enhanced_wiener_mel[0], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"IRM과 Wiener 필터 적용 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12번째 샘플 데이터 준비\n",
    "sample_index = 12\n",
    "\n",
    "# Mix 샘플: Wiener 필터 적용 후 Mel-spectrogram\n",
    "mix_sample = torch.tensor(enhanced_wiener_mel[sample_index]).unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, Time, Mel)\n",
    "clean_sample = clean_mel_list[sample_index].unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, Time, Mel)\n",
    "\n",
    "# UNet 모델 준비\n",
    "model = UNet().to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 루프 설정\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "print(\"Starting regression training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # UNet 모델 출력\n",
    "    output = model(mix_sample)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(output, clean_sample)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 손실 기록\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 10 epoch마다 손실 출력\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# 학습 완료 후 손실 시각화\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Regression Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델의 출력 Mel-spectrogram 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_clean = model(mix_sample).squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "# 원본 clean과 비교하여 시각화\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Clean Mel\")\n",
    "plt.imshow(clean_sample.squeeze(0).squeeze(0).cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Input Noisy Mel (Wiener Filter)\")\n",
    "plt.imshow(mix_sample.squeeze(0).squeeze(0).cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Clean Mel\")\n",
    "plt.imshow(predicted_clean, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Regression training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 재구성\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "print(\"Starting regression training with detailed loss outputs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # UNet 모델 출력\n",
    "    output = model(mix_sample)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(output, clean_sample)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 손실 기록\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 10 epoch마다 손실 출력\n",
    "    if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# 학습 완료 후 전체 손실 시각화\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Regression Training Loss (Detailed Output)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 학습된 모델의 출력 Mel-spectrogram 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_clean = model(mix_sample).squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "# 원본 clean과 비교하여 시각화\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Clean Mel\")\n",
    "plt.imshow(clean_sample.squeeze(0).squeeze(0).cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Input Noisy Mel (Wiener Filter)\")\n",
    "plt.imshow(mix_sample.squeeze(0).squeeze(0).cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Clean Mel\")\n",
    "plt.imshow(predicted_clean, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss output and spectrogram visualization completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hifigan 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "\n",
    "# HiFi-GAN 모델 경로 설정\n",
    "local_model_path = '/data/hjhan/lab/Audio MNIST/hifigan'\n",
    "\n",
    "# HiFi-GAN 모델 로드\n",
    "hifi_gan = HIFIGAN.from_hparams(\n",
    "    source=local_model_path,\n",
    "    savedir=local_model_path\n",
    ")\n",
    "\n",
    "# 혼합된 멜 스펙트로그램을 HiFi-GAN으로 파형으로 복원\n",
    "reconstructed_waveform = hifi_gan.decode_batch(mel_spectrogram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "# 12번째 샘플 선택\n",
    "sample_idx = 11  # 12번째 샘플 (0-based indexing)\n",
    "\n",
    "# 재구성된 파형을 12번째 샘플에 대해 가져옴\n",
    "reconstructed_waveform_sample = reconstructed_waveform[sample_idx].squeeze().cpu().detach().numpy()\n",
    "\n",
    "# 오디오 출력 (재구성된 파형)\n",
    "Audio(reconstructed_waveform_sample, rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystoi import stoi\n",
    "\n",
    "# 12번째 샘플의 clean waveform을 불러옴\n",
    "clean_waveform_sample = clean_waveforms[sample_idx].squeeze().cpu().detach().numpy()\n",
    "\n",
    "# STOI 계산\n",
    "stoi_value = stoi(clean_waveform_sample, reconstructed_waveform_sample, 16000)\n",
    "print(f\"STOI (Speech Transmission Index) for 12th sample: {stoi_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Mel-spectrogram 계산 (시각적인 품질 비교를 위한 준비)\n",
    "def compute_mel_spectrogram(waveform, sr=16000, n_mels=128, hop_length=512, win_length=1024):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "    return mel_spectrogram\n",
    "\n",
    "# clean 및 reconstructed waveform을 Mel-spectrogram으로 변환\n",
    "clean_mel = compute_mel_spectrogram(clean_waveform_sample)\n",
    "reconstructed_mel = compute_mel_spectrogram(reconstructed_waveform_sample)\n",
    "\n",
    "# SSIM 계산\n",
    "ssim_value = ssim(clean_mel, reconstructed_mel)\n",
    "print(f\"SSIM (Structural Similarity Index) for 12th sample: {ssim_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFi-GAN으로 복원된 파형을 다시 출력 (12번째 샘플)\n",
    "reconstructed_waveform_12th_sample = hifi_gan.decode_batch(reconstructed_mel)\n",
    "\n",
    "# 재구성된 파형 시청\n",
    "Audio(reconstructed_waveform_12th_sample.squeeze().cpu().detach().numpy(), rate=16000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
